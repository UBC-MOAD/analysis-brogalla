{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interracial-hypothetical",
   "metadata": {},
   "source": [
    "# ANHA4 to ANHA12 grid mapping\n",
    "\n",
    "Using Scipy griddata for the interpolation, joblib Parallel to split jobs over multiple cores, and xarray to read and write to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tamil-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import time\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "\n",
    "# Library for running on multiple cores:\n",
    "from joblib import Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-pixel",
   "metadata": {},
   "source": [
    "Coordinate files to load, ANHA4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pursuant-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_ANHA4_gridT = nc.Dataset('/scratch/brogalla/ANHA4_files/ANHA4-EXH005_y2002m01d05_gridT.nc')\n",
    "nc_ANHA4_gridU = nc.Dataset('/scratch/brogalla/ANHA4_files/ANHA4-EXH005_y2002m01d05_gridU.nc')\n",
    "nc_ANHA4_gridV = nc.Dataset('/scratch/brogalla/ANHA4_files/ANHA4-EXH005_y2002m01d05_gridV.nc')\n",
    "nc_ANHA4_gridW = nc.Dataset('/scratch/brogalla/ANHA4_files/ANHA4-EXH005_y2002m01d05_gridW.nc')\n",
    "nc_ANHA4_icemod = nc.Dataset('/scratch/brogalla/ANHA4_files/ANHA4-EXH005_y2002m01d05_icemod.nc')\n",
    "lon_ANHA4_gridT = np.array(nc_ANHA4_gridT.variables['nav_lon']); lat_ANHA4_gridT = np.array(nc_ANHA4_gridT.variables['nav_lat']);\n",
    "lon_ANHA4_gridU = np.array(nc_ANHA4_gridU.variables['nav_lon']); lat_ANHA4_gridU = np.array(nc_ANHA4_gridU.variables['nav_lat']);\n",
    "lon_ANHA4_gridV = np.array(nc_ANHA4_gridV.variables['nav_lon']); lat_ANHA4_gridV = np.array(nc_ANHA4_gridV.variables['nav_lat']);\n",
    "lon_ANHA4_gridW = np.array(nc_ANHA4_gridW.variables['nav_lon']); lat_ANHA4_gridW = np.array(nc_ANHA4_gridW.variables['nav_lat']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-supplier",
   "metadata": {},
   "source": [
    "Coordinate files to load, ANHA12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "applicable-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_ANHA12_gridT = nc.Dataset('/home/brogalla/project/brogalla/GEOTRACES/data/ANHA12/ANHA12-EXH006_5d_gridT_y2002m01d05.nc')\n",
    "nc_ANHA12_gridU = nc.Dataset('/home/brogalla/project/brogalla/GEOTRACES/data/ANHA12/ANHA12-EXH006_5d_gridU_y2002m01d05.nc')\n",
    "nc_ANHA12_gridV = nc.Dataset('/home/brogalla/project/brogalla/GEOTRACES/data/ANHA12/ANHA12-EXH006_5d_gridV_y2002m01d05.nc')\n",
    "nc_ANHA12_gridW = nc.Dataset('/home/brogalla/project/brogalla/GEOTRACES/data/ANHA12/ANHA12-EXH006_5d_gridW_y2002m01d05.nc')\n",
    "nc_ANHA12_icemod = nc.Dataset('/home/brogalla/project/brogalla/GEOTRACES/data/ANHA12/ANHA12-EXH006_5d_icemod_y2002m01d05.nc')\n",
    "lon_ANHA12_gridT = np.array(nc_ANHA12_gridT.variables['nav_lon']); lat_ANHA12_gridT = np.array(nc_ANHA12_gridT.variables['nav_lat']);\n",
    "lon_ANHA12_gridU = np.array(nc_ANHA12_gridU.variables['nav_lon']); lat_ANHA12_gridU = np.array(nc_ANHA12_gridU.variables['nav_lat']);\n",
    "lon_ANHA12_gridV = np.array(nc_ANHA12_gridV.variables['nav_lon']); lat_ANHA12_gridV = np.array(nc_ANHA12_gridV.variables['nav_lat']);\n",
    "lon_ANHA12_gridW = np.array(nc_ANHA12_gridW.variables['nav_lon']); lat_ANHA12_gridW = np.array(nc_ANHA12_gridW.variables['nav_lat']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rolled-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_np(nav_lon, nav_lat, var_in, lon_ANHA12, lat_ANHA12):\n",
    "    ''' Interpolate some field to ANHA12 grid.\n",
    "        The function is based on the bilinear interpolation in scipy, griddata \n",
    "        =======================================================================\n",
    "            nav_lon, nav_lat        : input field lon/lat\n",
    "            lon_ANHA12, lat_ANHA12  : ANHA12 defined lons/lats\n",
    "            var_in                  : 2-D model variable\n",
    "    '''\n",
    "    from scipy.interpolate import griddata\n",
    "    \n",
    "    LatLonPair = (nav_lon, nav_lat)\n",
    "    var_out = griddata(LatLonPair, var_in, (lon_ANHA12, lat_ANHA12), method='linear')\n",
    "    return var_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comfortable-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_gridT_xr(filenameT, lon_ANHA4=lon_ANHA4_gridT, lat_ANHA4=lat_ANHA4_gridT, lon_ANHA12=lon_ANHA12_gridT, lat_ANHA12=lat_ANHA12_gridT):\n",
    "    # Load file\n",
    "    file    = xr.open_dataset('/data/brogalla/ANHA4/' + filenameT)\n",
    "    varT    = file['votemper'].values\n",
    "    varS    = file['vosaline'].values\n",
    "    varx    = file['somxl010'].values\n",
    "    \n",
    "    # Interpolate ANHA4 variables onto ANHA12 grid:\n",
    "    ANHA12_votemper = np.empty((1,50,2400,1632))\n",
    "    ANHA12_vosaline = np.empty((1,50,2400,1632))\n",
    "    for depth in range(0,50):\n",
    "        ANHA12_votemper[0,depth,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varT[0,depth,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "        ANHA12_vosaline[0,depth,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varS[0,depth,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "    \n",
    "    ANHA12_somxl010 = np.empty((1,2400,1632))    \n",
    "    ANHA12_somxl010[0,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varx[0,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "    \n",
    "    # Write interpolated values to file:\n",
    "    file_write = xr.Dataset(\n",
    "        {'votemper': ((\"time_counter\",\"deptht\",\"y\",\"x\"), ANHA12_votemper),\n",
    "         'vosaline': ((\"time_counter\",\"deptht\",\"y\",\"x\"), ANHA12_vosaline),\n",
    "         'somxl010': ((\"time_counter\",\"y\",\"x\"), ANHA12_somxl010)\n",
    "        }, \n",
    "        coords = {\n",
    "            \"time_counter\": np.zeros(1),\n",
    "            \"deptht\": np.zeros(50),\n",
    "            \"y\": np.zeros(2400),\n",
    "            \"x\": np.zeros(1632),\n",
    "        },\n",
    "    )\n",
    "    file_write.to_netcdf('/scratch/brogalla/ANHA4_remapped/ANHA4-EXH005_5d_gridT_'+filenameT[13:24]+'.nc')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_gridT_nc(filenameT, lon_ANHA4=lon_ANHA4_gridT, lat_ANHA4=lat_ANHA4_gridT, lon_ANHA12=lon_ANHA12_gridT, lat_ANHA12=lat_ANHA12_gridT):\n",
    "    # Load file\n",
    "    file    = nc.Dataset('/data/brogalla/ANHA4/' + filenameT)\n",
    "    varT    = np.array(file.variables['votemper'])\n",
    "    varS    = np.array(file.variables['vosaline'])\n",
    "    varx    = np.array(file.variables['somxl010'])\n",
    "\n",
    "    # Interpolate ANHA4 variables onto ANHA12 grid:\n",
    "    ANHA12_votemper = np.empty((1,50,2400,1632))\n",
    "    ANHA12_vosaline = np.empty((1,50,2400,1632))\n",
    "    for depth in range(0,50):\n",
    "        ANHA12_votemper[0,depth,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varT[0,depth,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "        ANHA12_vosaline[0,depth,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varS[0,depth,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "    \n",
    "    ANHA12_somxl010 = np.empty((1,2400,1632))    \n",
    "    ANHA12_somxl010[0,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varx[0,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "    \n",
    "    # Write interpolated values to file:\n",
    "    file_write = xr.Dataset(\n",
    "        {'votemper': ((\"time_counter\",\"deptht\",\"y\",\"x\"), ANHA12_votemper),\n",
    "         'vosaline': ((\"time_counter\",\"deptht\",\"y\",\"x\"), ANHA12_vosaline),\n",
    "         'somxl010': ((\"time_counter\",\"y\",\"x\"), ANHA12_somxl010)\n",
    "        }, \n",
    "        coords = {\n",
    "            \"time_counter\": np.zeros(1),\n",
    "            \"deptht\": np.zeros(50),\n",
    "            \"y\": np.zeros(2400),\n",
    "            \"x\": np.zeros(1632),\n",
    "        },\n",
    "    )\n",
    "    file_write.to_netcdf('/scratch/brogalla/ANHA4_remapped/ANHA4-EXH005_5d_gridT_'+filenameT[13:24]+'.nc')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_gridT_nowrite(filenameT, lon_ANHA4=lon_ANHA4_gridT, lat_ANHA4=lat_ANHA4_gridT, lon_ANHA12=lon_ANHA12_gridT, lat_ANHA12=lat_ANHA12_gridT):\n",
    "    # Load file\n",
    "    file    = xr.open_dataset('/data/brogalla/ANHA4/' + filenameT)\n",
    "    varT    = file['votemper'].values\n",
    "    varS    = file['vosaline'].values\n",
    "    varx    = file['somxl010'].values\n",
    "\n",
    "    # Interpolate ANHA4 variables onto ANHA12 grid:\n",
    "    ANHA12_votemper = np.empty((1,50,2400,1632))\n",
    "    ANHA12_vosaline = np.empty((1,50,2400,1632))\n",
    "    for depth in range(0,50):\n",
    "        ANHA12_votemper[0,depth,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varT[0,depth,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "        ANHA12_vosaline[0,depth,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varS[0,depth,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "    \n",
    "    ANHA12_somxl010 = np.empty((1,2400,1632))    \n",
    "    ANHA12_somxl010[0,:,:] = interp_np(lon_ANHA4.flatten(), lat_ANHA4.flatten(), varx[0,:,:].flatten(), lon_ANHA12, lat_ANHA12)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legitimate-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joblib solver such that it passes a file to the main calculation and returns what you want\n",
    "def joblib_solver(main_calc, file):\n",
    "    calc = main_calc(file)\n",
    "    return calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "activated-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridT_files = ['ANHA4-EXH005_y2002m01d05_gridT.nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lonely-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add items to the list of jobs that need to be calculated. Each job reads in a file and performs a calculation on it.\n",
    "joblist_nc=[]\n",
    "joblist_xr=[]\n",
    "joblist_nowrite=[]\n",
    "\n",
    "for file in gridT_files:\n",
    "    positional_args_nc = [interp_gridT_nc, file]\n",
    "    positional_args_xr = [interp_gridT_xr, file]\n",
    "    positional_args_nowrite = [interp_gridT_nowrite, file]\n",
    "    \n",
    "    keyword_args={}\n",
    "    \n",
    "    joblist_nc.append((joblib_solver,positional_args_nc,keyword_args))\n",
    "    joblist_xr.append((joblib_solver,positional_args_xr,keyword_args))\n",
    "    joblist_nowrite.append((joblib_solver,positional_args_nowrite,keyword_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-reason",
   "metadata": {},
   "source": [
    "Check that the interpolation gives reasonable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridT file calculation took: 0.0000 mins with 1 core for 1 file using netCDF4 to read, and xarray to write to file.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "ncores=1\n",
    "with Parallel(n_jobs=ncores,backend='threading') as parallel:\n",
    "    parallel(joblist_nc)\n",
    "    \n",
    "end = time.time()\n",
    "print('GridT file calculation took: {:.4f} mins with 1 core for 1 file using netCDF4 to read, and xarray to write to file.'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "established-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridT file calculation took:  10.53926839431127 mins, with 1 core for 1 file using xarray and writing to file.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "ncores=1\n",
    "with Parallel(n_jobs=ncores,backend='threading') as parallel:\n",
    "    parallel(joblist_xr)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('GridT file calculation took: {:.4f} mins with 1 core for 1 file using xarray and writing to file.'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "considerable-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridT file calculation took:  10.022475628058116 mins, with 1 core for 1 file using xarray and not writing to file.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    " \n",
    "ncores=1\n",
    "with Parallel(n_jobs=ncores,backend='threading') as parallel:\n",
    "    parallel(joblist_nowrite)\n",
    "    \n",
    "end = time.time()\n",
    "print('GridT file calculation took: {:.4f} mins with 1 core for 1 file using xarray and not writing to file.'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-cologne",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
