{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAA Mn content evolution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "from itertools import compress\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain dimensions:\n",
    "imin, imax = 1479, 2179\n",
    "jmin, jmax = 159, 799\n",
    "isize = imax - imin\n",
    "jsize = jmax - jmin\n",
    "\n",
    "# Results folders:\n",
    "folder_ref   = '/data/brogalla/run_storage/Mn-reference-202110/'\n",
    "folder_clean = '/data/brogalla/run_storage/Mn-clean-ice-202110/'\n",
    "folder_glac  = '/data/brogalla/run_storage/river-glacial-202112/'\n",
    "folder_cont  = '/data/brogalla/run_storage/river-continental-202112/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh       = nc.Dataset('/ocean/brogalla/GEOTRACES/data/ANHA12/ANHA12_mesh1.nc')\n",
    "lon        = np.array(mesh.variables['nav_lon'])\n",
    "lat        = np.array(mesh.variables['nav_lat'])\n",
    "mesh_bathy = np.array(mesh.variables['hdept'][0])\n",
    "tmask      = np.array(mesh.variables['tmask'])[0,:,imin:imax,jmin:jmax]\n",
    "land_mask  = np.ma.masked_where((tmask[:,:,:] > 0.1), tmask[:,:,:]) \n",
    "e1t_base   = np.array(mesh.variables['e1t'])[0,imin:imax,jmin:jmax]\n",
    "e2t_base   = np.array(mesh.variables['e2t'])[0,imin:imax,jmin:jmax]\n",
    "e3t        = np.array(mesh.variables['e3t_0'])[0,:,imin:imax,jmin:jmax]\n",
    "e3t_masked = np.ma.masked_where((tmask[:,:,:] < 0.1), e3t)\n",
    "nav_lev    = np.array(mesh.variables['nav_lev'])\n",
    "\n",
    "e1t        = np.tile(e1t_base, (50,1,1))\n",
    "e2t        = np.tile(e2t_base, (50,1,1))\n",
    "volume     = e1t*e2t*e3t\n",
    "area_base  = e1t_base*e2t_base\n",
    "volume_masked = np.ma.masked_where((tmask[:,:,:] < 0.1), volume)\n",
    "area_masked   = np.ma.masked_where((tmask[0,:,:] < 0.1), area_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref   = nc.Dataset(f'{folder_ref}ANHA12_ref-2002_20211012/ANHA12_EXH006_2002_monthly.nc',  'r')\n",
    "lons  = np.array(ref.variables['nav_lon'])\n",
    "lats  = np.array(ref.variables['nav_lat'])\n",
    "depth = np.array(ref.variables['deptht'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff(year, glac=False, cont=False, nosed=False):\n",
    "    \n",
    "    if glac:\n",
    "        files_base  = np.sort(glob.glob(f'{folder_ref}ANHA12_ref-{year}_20211012/ANHA12_EXH006_5d*'))\n",
    "        files_glac  = np.sort(glob.glob(f'{folder_glac}ANHA12_glacial-{year}_20211130/ANHA12_EXH006_5d*'))\n",
    "        \n",
    "        glacier_contribution   = np.empty((len(files_base),50,isize,jsize))\n",
    "        for file_ind in range(0,len(files_base)):\n",
    "            with xr.open_dataset(files_base[file_ind]) as base_fday:\n",
    "                dmn_base  = base_fday['dissolmn'].values[0,:,:,:]\n",
    "            with xr.open_dataset(files_glac[file_ind]) as glac_fday:\n",
    "                dmn_glac  = glac_fday['dissolmn'].values[0,:,:,:]\n",
    "                \n",
    "            glacier_contribution[file_ind,:,:,:]   = dmn_glac - dmn_base # units: mol / L \n",
    "            \n",
    "        return glacier_contribution\n",
    "   \n",
    "    elif cont:\n",
    "        files_base  = np.sort(glob.glob(f'{folder_ref}ANHA12_ref-{year}_20211012/ANHA12_EXH006_5d*'))\n",
    "        files_cont  = np.sort(glob.glob(f'{folder_cont}ANHA12_continental-{year}_20211130/ANHA12_EXH006_5d*'))\n",
    "        \n",
    "        continent_contribution = np.empty((len(files_base),50,isize,jsize))\n",
    "        \n",
    "        for file_ind in range(0,len(files_base)):\n",
    "            with xr.open_dataset(files_base[file_ind]) as base_fday:\n",
    "                dmn_base  = base_fday['dissolmn'].values[0,:,:,:]\n",
    "            with xr.open_dataset(files_cont[file_ind]) as cont_fday:\n",
    "                dmn_cont  = cont_fday['dissolmn'].values[0,:,:,:]\n",
    "            \n",
    "            continent_contribution[file_ind,:,:,:] = dmn_cont - dmn_base\n",
    "            \n",
    "        return continent_contribution\n",
    "                \n",
    "    elif nosed:\n",
    "        files_base  = np.sort(glob.glob(f'{folder_ref}ANHA12_ref-{year}_20211012/ANHA12_EXH006_5d*'))\n",
    "        files_nosed = np.sort(glob.glob(f'{folder_clean}ANHA12_clean-ice-{year}_20211023/ANHA12_EXH006_5d*'))\n",
    "        \n",
    "        sediment_contribution  = np.empty((len(files_base),50,isize,jsize))\n",
    "    \n",
    "    \n",
    "        for file_ind in range(0,len(files_base)):\n",
    "            with xr.open_dataset(files_base[file_ind]) as base_fday:\n",
    "                dmn_base  = base_fday['dissolmn'].values[0,:,:,:]\n",
    "            with xr.open_dataset(files_nosed[file_ind]) as nosed_fday:\n",
    "                dmn_nosed  = nosed_fday['dissolmn'].values[0,:,:,:]\n",
    "\n",
    "            sediment_contribution[file_ind,:,:,:]  = dmn_base - dmn_nosed\n",
    "            \n",
    "        return sediment_contribution\n",
    "    else:\n",
    "        print('choose an experiment from glac, cont, nosed')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_dates(year):\n",
    "    #start_date and end_date are datetime objects\n",
    "    start_date = dt.datetime(year,1,1)\n",
    "    end_date   = dt.datetime(year,12,31,12)\n",
    "    \n",
    "    file_list1E = np.sort(os.listdir(f'{folder_ref}ANHA12_ref-{year}_20211012/'))\n",
    "    file_list2 = np.sort(os.listdir('/data/brogalla/ANHA12/'))\n",
    "    \n",
    "    Vlist = [i[26:31]=='gridV' for i in file_list2]\n",
    "    gridV_list = list(compress(file_list2, Vlist))\n",
    "    dateV_list = [dt.datetime.strptime(i[14:25], \"y%Ym%md%d\") for i in gridV_list]\n",
    "    gridV_file_list = list(compress(gridV_list, [V > start_date and V < end_date for V in dateV_list]))\n",
    "    \n",
    "    dates = [dt.datetime.strptime(i[14:25], \"y%Ym%md%d\") for i in gridV_file_list]\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mask for CAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ini_CAA  = np.ones(lon.shape) # full domain size\n",
    "mask_ini_CAA[0:imin+10, :] = 0 # remove the boundaries\n",
    "mask_ini_CAA[imax-10:, :]  = 0\n",
    "mask_ini_CAA[:, 0:jmin]    = 0\n",
    "mask_ini_CAA[:, jmax-20:]  = 0\n",
    "\n",
    "# Define CAA using the longitudes\n",
    "mask_CAA = np.ma.masked_where((mesh_bathy >= 800), mask_ini_CAA)\n",
    "mask_CAA = np.ma.masked_where(lon < -130, mask_CAA)\n",
    "\n",
    "mask_CAA_lons = np.ma.masked_where(mask_CAA==0, lon)\n",
    "mask_CAA_lats = np.ma.masked_where(mask_CAA==0, lat)\n",
    "\n",
    "mask_CAA_yr   = np.tile(mask_CAA[imin:imax,jmin:jmax], (73,50,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And mask for Canada Basin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ini_CB  = np.ones(lon.shape) # full domain size\n",
    "mask_ini_CB[0:imin+10, :] = 0 # remove the boundaries\n",
    "mask_ini_CB[imax-10:, :]  = 0\n",
    "mask_ini_CB[:, 0:jmin]    = 0\n",
    "mask_ini_CB[:, jmax-20:]  = 0\n",
    "\n",
    "# Define Canada Basin using the 800 m contour and longitudes\n",
    "mask_CB = np.ma.masked_where((mesh_bathy < 3200), mask_ini_CB)\n",
    "mask_CB = np.ma.masked_where((lon > -80), mask_CB)\n",
    "\n",
    "mask_CB_lons = np.ma.masked_where(mask_CB==0, lon)\n",
    "mask_CB_lats = np.ma.masked_where(mask_CB==0, lat)\n",
    "\n",
    "mask_CB_yr   = np.tile(mask_CB[imin:imax,jmin:jmax], (73,50,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ini_fullCB  = np.ones(lon.shape) # full domain size\n",
    "mask_ini_fullCB[0:imin+10, :] = 0 # remove the boundaries\n",
    "mask_ini_fullCB[imax-10:, :]  = 0\n",
    "mask_ini_fullCB[:, 0+10:jmin] = 0\n",
    "mask_ini_fullCB[:, jmax-20:]  = 0\n",
    "\n",
    "# Define fullCB using the longitudes\n",
    "mask_fullCB = np.ma.masked_where((mesh_bathy < 800), mask_ini_fullCB)\n",
    "mask_fullCB = np.ma.masked_where((lon > -60), mask_fullCB)\n",
    "mask_fullCB = np.ma.masked_where((lon > -100) & (lat < 78), mask_fullCB)\n",
    "\n",
    "mask_fullCB_lons = np.ma.masked_where(mask_fullCB==0, lon)\n",
    "mask_fullCB_lats = np.ma.masked_where(mask_fullCB==0, lat)\n",
    "\n",
    "mask_fullCB_yr   = np.tile(mask_fullCB[imin:imax,jmin:jmax], (73,50,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the glacial time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "timeseries_glac_CAA = []\n",
    "timeseries_glac_CBf = []\n",
    "timeseries_glac_CBc = []\n",
    "\n",
    "for year in range(2002, 2020):\n",
    "    print(year)\n",
    "    glacier = calculate_diff(year, glac=True)\n",
    "    glacier_CAA = np.ma.masked_where(mask_CAA_yr==0   , glacier)\n",
    "    glacier_CBf = np.ma.masked_where(mask_fullCB_yr==0, glacier)\n",
    "    glacier_CBc = np.ma.masked_where(mask_CB_yr==0    , glacier)\n",
    "    \n",
    "    # multiply by gridbox volume before summing...\n",
    "    # (mol / L) * (m3) * (1000 L / m3) --> moles in the given region\n",
    "    for fday in range(0,73):        \n",
    "        timeseries_glac_CAA.append(np.ma.sum(glacier_CAA[fday,:,:,:]*volume_masked*1e3))\n",
    "        timeseries_glac_CBf.append(np.ma.sum(glacier_CBf[fday,:,:,:]*volume_masked*1e3))\n",
    "        timeseries_glac_CBc.append(np.ma.sum(glacier_CBc[fday,:,:,:]*volume_masked*1e3))\n",
    "        \n",
    "pickle.dump((timeseries_glac_CAA, timeseries_glac_CBf, timeseries_glac_CBc), \\\n",
    "            open('/ocean/brogalla/GEOTRACES/pickles/FWC_glacial_timeseries.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the continental time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "timeseries_cont_CAA = []\n",
    "timeseries_cont_CBf = []\n",
    "timeseries_cont_CBc = []\n",
    "\n",
    "for year in range(2002, 2020):\n",
    "    print(year)\n",
    "    continental = calculate_diff(year, cont=True)\n",
    "    continental_CAA = np.ma.masked_where(mask_CAA_yr==0   , continental)\n",
    "    continental_CBf = np.ma.masked_where(mask_fullCB_yr==0, continental)\n",
    "    continental_CBc = np.ma.masked_where(mask_CB_yr==0    , continental)\n",
    "    \n",
    "    # multiply by gridbox volume before summing...\n",
    "    # (mol / L) * (m3) * (1000 L / m3) --> moles in the given region\n",
    "    for fday in range(0,73):        \n",
    "        timeseries_cont_CAA.append(np.ma.sum(continental_CAA[fday,:,:,:]*volume_masked*1e3))\n",
    "        timeseries_cont_CBf.append(np.ma.sum(continental_CBf[fday,:,:,:]*volume_masked*1e3))\n",
    "        timeseries_cont_CBc.append(np.ma.sum(continental_CBc[fday,:,:,:]*volume_masked*1e3))\n",
    "        \n",
    "pickle.dump((timeseries_cont_CAA, timeseries_cont_CBf, timeseries_cont_CBc), \\\n",
    "            open('/ocean/brogalla/GEOTRACES/pickles/FWC_continental_timeseries.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea ice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "timeseries_nosed_CAA = []\n",
    "timeseries_nosed_CBf = []\n",
    "timeseries_nosed_CBc = []\n",
    "\n",
    "for year in range(2002, 2020):\n",
    "    print(year)\n",
    "    cleanice = calculate_diff(year, nosed=True)\n",
    "    cleanice_CAA = np.ma.masked_where(mask_CAA_yr==0   , cleanice)\n",
    "    cleanice_CBf = np.ma.masked_where(mask_fullCB_yr==0, cleanice)\n",
    "    cleanice_CBc = np.ma.masked_where(mask_CB_yr==0    , cleanice)\n",
    "    \n",
    "    # multiply by gridbox volume before summing...\n",
    "    # (mol / L) * (m3) * (1000 L / m3) --> moles in the given region\n",
    "    for fday in range(0,73):        \n",
    "        timeseries_nosed_CAA.append(np.ma.sum(cleanice_CAA[fday,:,:,:]*volume_masked*1e3))\n",
    "        timeseries_nosed_CBf.append(np.ma.sum(cleanice_CBf[fday,:,:,:]*volume_masked*1e3))\n",
    "        timeseries_nosed_CBc.append(np.ma.sum(cleanice_CBc[fday,:,:,:]*volume_masked*1e3))\n",
    "        \n",
    "pickle.dump((timeseries_nosed_CAA, timeseries_nosed_CBf, timeseries_nosed_CBc), \\\n",
    "            open('/ocean/brogalla/GEOTRACES/pickles/FWC_cleanice_timeseries.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
